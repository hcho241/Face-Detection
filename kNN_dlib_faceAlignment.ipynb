{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = cv2.imread(\"emily.jpg\")\n",
    "\n",
    "# Convert to dlib\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# dlib face detection\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "detections = detector(img, 1)\n",
    "\n",
    "# Find landmarks\n",
    "sp = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "faces = dlib.full_object_detections()\n",
    "for det in detections:\n",
    "    faces.append(sp(img, det))\n",
    "\n",
    "# Bounding box and eyes\n",
    "bb = [i.rect for i in faces]\n",
    "bb = [((i.left(), i.top()),\n",
    "       (i.right(), i.bottom())) for i in bb]                            # Convert out of dlib format\n",
    "\n",
    "right_eyes = [[face.part(i) for i in range(36, 42)] for face in faces]\n",
    "right_eyes = [[(i.x, i.y) for i in eye] for eye in right_eyes]          # Convert out of dlib format\n",
    "\n",
    "left_eyes = [[face.part(i) for i in range(42, 48)] for face in faces]\n",
    "left_eyes = [[(i.x, i.y) for i in eye] for eye in left_eyes]            # Convert out of dlib format\n",
    "\n",
    "# Display\n",
    "imgd = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)             # Convert back to OpenCV\n",
    "for i in bb:\n",
    "    cv2.rectangle(imgd, i[0], i[1], (255, 0, 0), 5)     # Bounding box\n",
    "\n",
    "for eye in right_eyes:\n",
    "    cv2.rectangle(imgd, (max(eye, key=lambda x: x[0])[0], max(eye, key=lambda x: x[1])[1]),\n",
    "                        (min(eye, key=lambda x: x[0])[0], min(eye, key=lambda x: x[1])[1]),\n",
    "                        (0, 0, 255), 5)\n",
    "    for point in eye:\n",
    "        cv2.circle(imgd, (point[0], point[1]), 2, (0, 255, 0), -1)\n",
    "\n",
    "for eye in left_eyes:\n",
    "    cv2.rectangle(imgd, (max(eye, key=lambda x: x[0])[0], max(eye, key=lambda x: x[1])[1]),\n",
    "                        (min(eye, key=lambda x: x[0])[0], min(eye, key=lambda x: x[1])[1]),\n",
    "                        (0, 255, 0), 5)\n",
    "    for point in eye:\n",
    "        cv2.circle(imgd, (point[0], point[1]), 2, (0, 0, 255), -1)\n",
    "\n",
    "#cv2.imwrite(\"output.jpg\", imgd)\n",
    "\n",
    "cv2.imshow(\"output\", imgd)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() # close the window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "class FaceAligner:\n",
    "    def __init__(self, predictor, desiredLeftEye=(0.35, 0.35), desiredFaceWidth=256, desiredFaceHeight=None) :\n",
    "        # store the facial landmark predictor, desired output left eye position, and desired output face width + height\n",
    "        self.predictor = predictor\n",
    "        self.desiredLeftEye = desiredLeftEye\n",
    "        self.desiredFaceWidth = desiredFaceWidth\n",
    "        self.desiredFaceHeight = desiredFaceHeight\n",
    "        # if the desired face height is None, set it to be the desired face width (normal behavior)\n",
    "        if self.desiredFaceHeight is None:\n",
    "            self.desiredFaceHeight = self.desiredFaceWidth\n",
    "    def align(self, image, gray, rect) :\n",
    "        # convert the landmark (x, y)-coordinates to a NumPy array\n",
    "        shape = self.predictor(gray, rect)\n",
    "        shape = shape_to_np(shape)\n",
    "        \n",
    "        # extract the left and right eye (x, y)-coordinates\n",
    "        (lStart, lEnd) = FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "        (rStart, rEnd) = FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
    "        leftEyePts = shape[lStart:lEnd]\n",
    "        rightEyePts = shape[rStart:rEnd]\n",
    "        \n",
    "        # compute the center of mass for each eye\n",
    "        leftEyeCenter = leftEyePts.mean(axis=0).astype(\"int\")\n",
    "        rightEyeCenter = rightEyePts.mean(axis=0).astype(\"int\")\n",
    "        \n",
    "        # compute the angle between the eye centroids\n",
    "        dY = rightEyeCenter[1] - leftEyeCenter[1]\n",
    "        dX = rightEyeCenter[0] - leftEyeCenter[0]\n",
    "        angle = np.degrees(np.arctan2(dY, dX)) - 180\n",
    "        \n",
    "        # compute the desired right eye x-coordinate based on the desired x-coordinate of the left eye\n",
    "        desiredRightEyeX = 1.0 - self.desiredLeftEye[0]\n",
    "        \n",
    "        # determine the scale of the new resulting image by taking the ratio of the distance between eyes in the *current*\n",
    "        # image to the ratio of distance between eyes in the *desired* image\n",
    "        dist = np.sqrt((dX ** 2) + (dY ** 2))\n",
    "        desiredDist = (desiredRightEyeX - self.desiredLeftEye[0])\n",
    "        desiredDist *= self.desiredFaceWidth\n",
    "        scale = desiredDist / dist\n",
    "        \n",
    "        # compute center (x, y)-coordinates (i.e., the median point) between the two eyes in the input image\n",
    "        eyesCenter = ((leftEyeCenter[0] + rightEyeCenter[0]) // 2, (leftEyeCenter[1] + rightEyeCenter[1]) // 2)\n",
    "        \n",
    "        # grab the rotation matrix for rotating and scaling the face\n",
    "        M = cv2.getRotationMatrix2D(eyesCenter, angle, scale)\n",
    "        \n",
    "        # update the translation component of the matrix\n",
    "        tX = self.desiredFaceWidth * 0.5\n",
    "        tY = self.desiredFaceHeight * self.desiredLeftEye[1]\n",
    "        M[0, 2] += (tX - eyesCenter[0])\n",
    "        M[1, 2] += (tY - eyesCenter[1])\n",
    "        \n",
    "        # apply the affine transformation\n",
    "        (w, h) = (self.desiredFaceWidth, self.desiredFaceHeight)\n",
    "        output = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC)\n",
    "        return output   # return the aligned face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import imutils\n",
    "import dlib\n",
    "from imutils.face_utils import FaceAligner\n",
    "from imutils.face_utils import rect_to_bb\n",
    "import glob\n",
    "import os, sys\n",
    "\n",
    "# Creating face_cascade object\n",
    "face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "dataset_path = './knn_dlib/'\n",
    "offset = 10\n",
    "\n",
    "# ===== create file_name folder under knn_dlib =====\n",
    "if not os.path.exists(dataset_path + \"/aligned\" ):\n",
    "    os.makedirs(dataset_path + \"/aligned\")\n",
    "\n",
    "# initialize dlib's face detector (HOG-based) and then create the facial landmark predictor and the face aligner\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "fa = FaceAligner(predictor, desiredFaceWidth=256)\n",
    "\n",
    "# load each image in folder \n",
    "filenames = [img for img in glob.glob(\"knn_dlib/hyFS/*.jpg\")]\n",
    "filenames.sort() # ADD THIS LINE\n",
    "images = []\n",
    "# load each image in folder, resize it, and convert it to grayscale\n",
    "for img in filenames:\n",
    "    image = cv2.imread(img)\n",
    "    image = imutils.resize(image, width=800)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # show the original input image and detect faces in the grayscale\n",
    "    # image\n",
    "    #cv2.imshow(\"Input\", image)\n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows() # close the window\n",
    "    rects = detector(gray, 2)\n",
    "    # loop over the face detections\n",
    "    for rect in rects:\n",
    "        # extract the ROI of the *original* face, then align the face\n",
    "        # using facial landmarks\n",
    "        (x, y, w, h) = rect_to_bb(rect)\n",
    "        faceOrig = imutils.resize(image[y:y + h, x:x + w], width=256)\n",
    "        faceAligned = fa.align(image, gray, rect)\n",
    "        #face_section = faceAligned[y-offset:y+h+offset, x-offset:x+w+offset]\n",
    "        #face_section = cv2.resize(face_section,(150, 150))\n",
    "    # save each aligned face \n",
    "    cv2.imwrite(img, faceAligned)\n",
    "    # display the output images\n",
    "    cv2.imshow(\"Original\", faceOrig)\n",
    "    cv2.imshow(\"Aligned\", faceAligned)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows() # close the window        \n",
    "\n",
    "'''\n",
    "# load sample image, resize it, and convert it to grayscale\n",
    "image = cv2.imread(\"emily.jpg\")\n",
    "image = imutils.resize(image, width=800)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "# show the original input image and detect faces in the grayscale\n",
    "# image\n",
    "cv2.imshow(\"Input\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() # close the window\n",
    "rects = detector(gray, 2)\n",
    "# loop over the face detections\n",
    "for rect in rects:\n",
    "    # extract the ROI of the *original* face, then align the face\n",
    "    # using facial landmarks\n",
    "    (x, y, w, h) = rect_to_bb(rect)\n",
    "    faceOrig = imutils.resize(image[y:y + h, x:x + w], width=256)\n",
    "    faceAligned = fa.align(image, gray, rect)\n",
    "# display the output images\n",
    "cv2.imshow(\"Original\", faceOrig)\n",
    "cv2.imshow(\"Aligned\", faceAligned)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() # close the window\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 2) load image and save its info as a key to compare ==========\n",
    "def get_face_encodings(face):\n",
    "    \"\"\"\n",
    "        return np.array of face recognition model which contains location, landmarks for face encoding\n",
    "    \"\"\"\n",
    "    bounds = face_detector(face, 1) # detect face rectangles \n",
    "    faces_landmarks = [shape_predictor(face, face_bounds) for face_bounds in bounds]\n",
    "    return [np.array(face_recognition_model.compute_face_descriptor(face, face_pose, 1)) for face_pose in faces_landmarks]\n",
    "\n",
    "def get_face_matches(known_faces, face):\n",
    "    \"\"\"\n",
    "        return euclidean distance \n",
    "    \"\"\"\n",
    "    return np.linalg.norm(known_faces - face, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_match(known_faces, person_name, face):\n",
    "    \"\"\"\n",
    "        min distance is the best prediction \n",
    "    \"\"\"\n",
    "    matches = get_face_matches(known_faces, face) \n",
    "    print('matches ', matches)\n",
    "    min_index = matches.argmin() # min distance index\n",
    "    print('min index ', min_index)\n",
    "    min_value = matches[min_index] # min distance\n",
    "    print('min value ', min_value)\n",
    "    matchPercent = 100 - (min_value * 100) # convert to percentage\n",
    "    print('matchPercent ', matchPercent, ' person name ', person_name)\n",
    "    if matchPercent >= 70 : # at least 80% of correction -> change to 70\n",
    "        return person_name +\" {0:.2f}%\".format(matchPercent)\n",
    "    return 'Not Found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_face_encodings(faces_folder_path):\n",
    "    \"\"\"\n",
    "        Load face images in person's name folder in a separate window \n",
    "    \"\"\"\n",
    "    image_filenames = filter(lambda x: x.endswith('.jpg'), os.listdir(faces_folder_path))\n",
    "    image_filenames = sorted(image_filenames)\n",
    "    person_names = []\n",
    "    for x in image_filenames :\n",
    "        #print('image file name ', x)\n",
    "        index = x.find('_')\n",
    "        person_names.append(x[:index]) # exclude from '_'\n",
    "    full_paths_to_images = [faces_folder_path + x for x in image_filenames]\n",
    "    print('full paths to images ', full_paths_to_images)\n",
    "    face_encodings = []\n",
    "    \n",
    "    for path_to_image in full_paths_to_images:\n",
    "        face = io.imread(path_to_image)\n",
    "        faces_bounds = face_detector(face, 1)\n",
    "        if len(faces_bounds) != 1:\n",
    "            print(\"Expected one and only one face per image: \" + path_to_image + \" - it has \" + str(len(faces_bounds)))\n",
    "        face_bounds = faces_bounds[0]\n",
    "        face_landmarks = shape_predictor(face, face_bounds)\n",
    "        face_encoding = np.array(face_recognition_model.compute_face_descriptor(face, face_landmarks, 1))\n",
    "        face_encodings.append(face_encoding)\n",
    "    #print('face encoding result ', face_encoding)\n",
    "\n",
    "    # ===== save this data into numpy array file & text file =====\n",
    "    np.save(dataset_path + person_names[0] + '.npy', face_encoding)\n",
    "    print(\"data successfully saved at \" + dataset_path + person_names[0] + '.npy')\n",
    "    return person_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation(data_dir) :\n",
    "    \"\"\"\n",
    "        Data Preparation by loading npy file \n",
    "    \"\"\"\n",
    "    face_data = []\n",
    "    for dataset in os.listdir(data_dir):\n",
    "        #print(\"Loaded \"+ dataset)\n",
    "        if dataset.endswith('.npy'):\n",
    "            data_item = np.load(dataset_path + dataset)\n",
    "            face_data.append(data_item)\n",
    "    return face_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 3) initialize webcam to compare key image features ==========\n",
    "\n",
    "data_dir = os.path.expanduser('./knn_dlib')\n",
    "faces_folder_path = data_dir + '/hy/'  # just need to change folder's name to encode \n",
    "        \n",
    "# ===== use timer to get how long it takes to load face encoding =====\n",
    "start_loading = time.time()\n",
    "person_name = load_face_encodings(faces_folder_path)\n",
    "end_loading = time.time()\n",
    "total_loading = end_loading - start_loading\n",
    "print('Took {0:.2f} seconds to load face encodings'.format(total_loading))\n",
    "\n",
    "# ===== load npy file =====\n",
    "face_data = data_preparation(data_dir)\n",
    "\n",
    "# ===== initialize webcam =====\n",
    "camera = cv2.VideoCapture(0)\n",
    "old_faces = []\n",
    "while True:\n",
    "    ret, frame = camera.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame = cv2.resize(frame, (0, 0), fx=0.5, fy=0.5)    \n",
    "    faces = face_detector(frame, 1)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    face_rects = face_cascade.detectMultiScale(gray, scaleFactor = 1.3, minNeighbors = 5, minSize = (50, 50), flags = cv2.CASCADE_SCALE_IMAGE)\n",
    "    if len(old_faces) < len(faces) :\n",
    "        old_faces = []\n",
    "        for face in faces :\n",
    "            tracker = dlib.correlation_tracker()\n",
    "            tracker.start_track(frame, face)\n",
    "            old_faces.append(tracker)\n",
    "    else :\n",
    "        for i, tracker in enumerate(old_faces) :\n",
    "            quality = tracker.update(frame)\n",
    "            if quality > 7 :\n",
    "                pos = tracker.get_position()\n",
    "                pos = dlib.rectangle(int(pos.left()), int(pos.top()), int(pos.right()), int(pos.bottom()))\n",
    "                face = frame[pos.top():pos.top() + pos.bottom(), pos.left():pos.left() + pos.right()]\n",
    "                start = time.time() \n",
    "                face_encodings_in_image = get_face_encodings(face)\n",
    "                if (face_encodings_in_image) :\n",
    "                    match = find_match(face_data, person_name, face_encodings_in_image[0])\n",
    "                    end = time.time()\n",
    "                    total = end - start\n",
    "                    print('Encoding Image Match Found took {0:.2f} seconds'.format(total))\n",
    "                    cv2.putText(frame, match, (pos.left()-50, pos.top()-15), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "                else : \n",
    "                    end = time.time()\n",
    "                    total = end - start\n",
    "                    print('Match Not Found took {0:.2f} seconds'.format(total))\n",
    "                    cv2.putText(frame, \"Unknown\", (pos.left()-15, pos.top()-15), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "                cv2.rectangle(frame, (pos.left(), pos.top()), (pos.right(), pos.bottom()), (0, 255, 255), 2)\n",
    "            else:\n",
    "                old_faces.pop(i)\n",
    "    cv2.imshow(\"frame\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q') :\n",
    "        break\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
